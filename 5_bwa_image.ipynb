{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476ded0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb4e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # keep if Tavily provides; DO NOT rely on it\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ImageSpec(BaseModel):\n",
    "    placeholder: str = Field(..., description=\"e.g. [[IMAGE_1]]\")\n",
    "    filename: str = Field(..., description=\"Save under images/, e.g. qkv_flow.png\")\n",
    "    alt: str\n",
    "    caption: str\n",
    "    prompt: str = Field(..., description=\"Prompt to send to the image model.\")\n",
    "    size: Literal[\"1024x1024\", \"1024x1536\", \"1536x1024\"] = \"1024x1024\"\n",
    "    quality: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n",
    "\n",
    "\n",
    "class GlobalImagePlan(BaseModel):\n",
    "    md_with_placeholders: str\n",
    "    images: List[ImageSpec] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9f8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "\n",
    "    # reducer/image\n",
    "    merged_md: str\n",
    "    md_with_placeholders: str\n",
    "    image_specs: List[dict]\n",
    "\n",
    "    final: str\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",   # latest powerful Groq model\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1c0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09e9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily)\n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "\n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
    "  If missing or unclear, set published_at=null. Do NOT guess.\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "\n",
    "    # take the first 10 queries from state\n",
    "    queries = (state.get(\"queries\", []) or [])\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(content=f\"Raw results:\\n{raw_results}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return {\"evidence\": list(dedup.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6c91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book:\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f634ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard:\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book:\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true:\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "\n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [(task.id, section_md)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea4856b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) ReducerWithImages (subgraph)\n",
    "#    merge_content -> decide_images -> generate_and_place_images\n",
    "# ============================================================\n",
    "def merge_content(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    merged_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "    return {\"merged_md\": merged_md}\n",
    "\n",
    "\n",
    "# DECIDE_IMAGES_SYSTEM = \"\"\"You are an expert technical editor.\n",
    "# Decide if images/diagrams are needed for THIS blog.\n",
    "\n",
    "# Rules:\n",
    "# - Max 3 images total.\n",
    "# - Each image must materially improve understanding (diagram/flow/table-like visual).\n",
    "# - Insert placeholders exactly: [[IMAGE_1]], [[IMAGE_2]], [[IMAGE_3]].\n",
    "# - If no images needed: md_with_placeholders must equal input and images=[].\n",
    "# - Avoid decorative images; prefer technical diagrams with short labels.\n",
    "# Return strictly GlobalImagePlan.\n",
    "# \"\"\"\n",
    "\n",
    "# def decide_images(state: State) -> dict:\n",
    "\n",
    "#     planner = llm.with_structured_output(GlobalImagePlan)\n",
    "#     merged_md = state[\"merged_md\"]\n",
    "#     plan = state[\"plan\"]\n",
    "#     assert plan is not None\n",
    "\n",
    "#     image_plan = planner.invoke(\n",
    "#         [\n",
    "#             SystemMessage(content=DECIDE_IMAGES_SYSTEM),\n",
    "#             HumanMessage(\n",
    "#                 content=(\n",
    "#                     f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "#                     f\"Topic: {state['topic']}\\n\\n\"\n",
    "#                     \"Insert placeholders + propose image prompts.\\n\\n\"\n",
    "#                     f\"{merged_md}\"\n",
    "#                 )\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     return {\n",
    "#         \"md_with_placeholders\": image_plan.md_with_placeholders,\n",
    "#         \"image_specs\": [img.model_dump() for img in image_plan.images],\n",
    "#     }\n",
    "\n",
    "\n",
    "# def _gemini_generate_image_bytes(prompt: str) -> bytes:\n",
    "#     \"\"\"\n",
    "#     Returns raw image bytes generated by Gemini.\n",
    "#     Requires: pip install google-genai\n",
    "#     Env var: GOOGLE_API_KEY\n",
    "#     \"\"\"\n",
    "#     from google import genai\n",
    "#     from google.genai import types\n",
    "\n",
    "#     api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "#     if not api_key:\n",
    "#         raise RuntimeError(\"GOOGLE_API_KEY is not set.\")\n",
    "\n",
    "#     client = genai.Client(api_key=api_key)\n",
    "\n",
    "#     resp = client.models.generate_content(\n",
    "#         model=\"gemini-2.5-flash-image\",\n",
    "#         contents=prompt,\n",
    "#         config=types.GenerateContentConfig(\n",
    "#             response_modalities=[\"IMAGE\"],\n",
    "#             safety_settings=[\n",
    "#                 types.SafetySetting(\n",
    "#                     category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "#                     threshold=\"BLOCK_ONLY_HIGH\",\n",
    "#                 )\n",
    "#             ],\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     # Depending on SDK version, parts may hang off resp.candidates[0].content.parts\n",
    "#     parts = getattr(resp, \"parts\", None)\n",
    "#     if not parts and getattr(resp, \"candidates\", None):\n",
    "#         try:\n",
    "#             parts = resp.candidates[0].content.parts\n",
    "#         except Exception:\n",
    "#             parts = None\n",
    "\n",
    "#     if not parts:\n",
    "#         raise RuntimeError(\"No image content returned (safety/quota/SDK change).\")\n",
    "\n",
    "#     for part in parts:\n",
    "#         inline = getattr(part, \"inline_data\", None)\n",
    "#         if inline and getattr(inline, \"data\", None):\n",
    "#             return inline.data\n",
    "\n",
    "#     raise RuntimeError(\"No inline image bytes found in response.\")\n",
    "\n",
    "\n",
    "# def generate_and_place_images(state: State) -> dict:\n",
    "\n",
    "#     plan = state[\"plan\"]\n",
    "#     assert plan is not None\n",
    "\n",
    "#     md = state.get(\"md_with_placeholders\") or state[\"merged_md\"]\n",
    "#     image_specs = state.get(\"image_specs\", []) or []\n",
    "\n",
    "#     # If no images requested, just write merged markdown\n",
    "#     if not image_specs:\n",
    "#         filename = f\"{plan.blog_title}.md\"\n",
    "#         Path(filename).write_text(md, encoding=\"utf-8\")\n",
    "#         return {\"final\": md}\n",
    "\n",
    "#     images_dir = Path(\"images\")\n",
    "#     images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#     for spec in image_specs:\n",
    "#         placeholder = spec[\"placeholder\"]\n",
    "#         filename = spec[\"filename\"]\n",
    "#         out_path = images_dir / filename\n",
    "\n",
    "#         # generate only if needed\n",
    "#         if not out_path.exists():\n",
    "#             try:\n",
    "#                 img_bytes = _gemini_generate_image_bytes(spec[\"prompt\"])\n",
    "#                 out_path.write_bytes(img_bytes)\n",
    "#             except Exception as e:\n",
    "#                 # graceful fallback: keep doc usable\n",
    "#                 prompt_block = (\n",
    "#                     f\"> **[IMAGE GENERATION FAILED]** {spec.get('caption','')}\\n>\\n\"\n",
    "#                     f\"> **Alt:** {spec.get('alt','')}\\n>\\n\"\n",
    "#                     f\"> **Prompt:** {spec.get('prompt','')}\\n>\\n\"\n",
    "#                     f\"> **Error:** {e}\\n\"\n",
    "#                 )\n",
    "#                 md = md.replace(placeholder, prompt_block)\n",
    "#                 continue\n",
    "\n",
    "#         img_md = f\"![{spec['alt']}](images/{filename})\\n*{spec['caption']}*\"\n",
    "#         md = md.replace(placeholder, img_md)\n",
    "\n",
    "#     filename = f\"{plan.blog_title}.md\"\n",
    "#     Path(filename).write_text(md, encoding=\"utf-8\")\n",
    "#     return {\"final\": md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebc44ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000220D10CD6A0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build reducer subgraph\n",
    "reducer_graph = StateGraph(State)\n",
    "reducer_graph.add_node(\"merge_content\", merge_content)\n",
    "# reducer_graph.add_node(\"decide_images\", decide_images)\n",
    "# reducer_graph.add_node(\"generate_and_place_images\", generate_and_place_images)\n",
    "# reducer_graph.add_edge(START, \"merge_content\")\n",
    "# reducer_graph.add_edge(\"merge_content\", \"decide_images\")\n",
    "# reducer_graph.add_edge(\"decide_images\", \"generate_and_place_images\")\n",
    "# reducer_graph.add_edge(\"generate_and_place_images\", END)\n",
    "\n",
    "reducer_graph.add_edge(START,\"merge_content\")\n",
    "reducer_graph.add_edge(\"merge_content\",END)\n",
    "reducer_subgraph = reducer_graph.compile()\n",
    "\n",
    "reducer_subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b41ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000220D1171450>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build main graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_subgraph)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f5a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str, as_of: Optional[str] = None):\n",
    "    if as_of is None:\n",
    "        as_of = date.today().isoformat()\n",
    "\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"as_of\": as_of,\n",
    "            \"recency_days\": 7,\n",
    "            \"sections\": [],\n",
    "            \"merged_md\": \"\",\n",
    "            \"md_with_placeholders\": \"\",\n",
    "            \"image_specs\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c066987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Self Attention in Transformer Architecture',\n",
       " 'mode': 'closed_book',\n",
       " 'needs_research': False,\n",
       " 'queries': [],\n",
       " 'evidence': [],\n",
       " 'plan': Plan(blog_title='Self Attention in Transformer Architecture', audience='developers', tone='technical', blog_kind='explainer', constraints=[], tasks=[Task(id=1, title='Introduction to Self Attention', goal='The reader should understand the concept of self attention and its role in transformer architecture.', bullets=['Define self attention and its purpose', 'Explain how self attention differs from traditional attention mechanisms', 'Discuss the benefits of using self attention in transformer models'], target_words=250, tags=['transformer', 'self attention'], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='Mathematical Formulation of Self Attention', goal='The reader should be able to understand the mathematical formulation of self attention.', bullets=['Derive the self attention equation', 'Explain the role of query, key, and value vectors', 'Discuss the importance of scaling in self attention'], target_words=300, tags=['math', 'transformer'], requires_research=False, requires_citations=False, requires_code=False), Task(id=3, title='Implementing Self Attention', goal='The reader should be able to implement self attention in a transformer model.', bullets=['Provide a minimal code sketch for self attention implementation', 'Explain the importance of masking in self attention', 'Discuss the use of multi-head attention'], target_words=350, tags=['implementation', 'transformer'], requires_research=False, requires_citations=False, requires_code=True), Task(id=4, title='Edge Cases and Failure Modes', goal='The reader should understand the edge cases and failure modes of self attention.', bullets=['Discuss the impact of sequence length on self attention', 'Explain the effects of sparse input on self attention', 'Describe the failure modes of self attention'], target_words=250, tags=['edge cases', 'failure modes'], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Performance and Cost Considerations', goal='The reader should understand the performance and cost considerations of self attention.', bullets=['Discuss the computational complexity of self attention', 'Explain the memory requirements of self attention', 'Describe the optimization techniques for self attention'], target_words=300, tags=['performance', 'cost'], requires_research=False, requires_citations=False, requires_code=False), Task(id=6, title='Security and Privacy Considerations', goal='The reader should understand the security and privacy considerations of self attention.', bullets=['Discuss the potential security risks of self attention', 'Explain the importance of data privacy in self attention', 'Describe the mitigation techniques for security and privacy risks'], target_words=250, tags=['security', 'privacy'], requires_research=False, requires_citations=False, requires_code=False), Task(id=7, title='Debugging and Observability Tips', goal='The reader should be able to debug and observe self attention in transformer models.', bullets=['Provide tips for debugging self attention', 'Explain the importance of visualization in self attention', 'Describe the use of logging and monitoring in self attention'], target_words=300, tags=['debugging', 'observability'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'sections': [(1,\n",
       "   '## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, allowing the model to focus on specific parts of the input data when generating outputs. It is defined as a mechanism that attends to different positions of a single sequence, weighing their importance, and computes a representation of the sequence based on this weighting. The purpose of self attention is to enable the model to capture long-range dependencies and contextual relationships within the input data.\\n\\nIn contrast to traditional attention mechanisms, self attention differs in that it does not rely on a separate encoder-decoder structure. Instead, self attention operates on a single sequence, allowing the model to attend to different parts of the input data simultaneously. This differs from traditional attention mechanisms, which typically attend to a separate context or memory.\\n\\nThe benefits of using self attention in transformer models are numerous. It enables the model to capture complex contextual relationships and dependencies within the input data, leading to improved performance on tasks such as language translation and text generation. Additionally, self attention allows the model to handle variable-length input sequences, making it well-suited for tasks such as machine translation and text summarization. Overall, self attention is a powerful tool that has revolutionized the field of natural language processing and has become a staple component of transformer architecture.'),\n",
       "  (2,\n",
       "   '## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand how self attention works, we need to derive the self attention equation. The self attention equation is derived from the concept of attention, which is calculated as the weighted sum of the value vectors, where the weights are computed based on the similarity between the query and key vectors.\\n\\n* The self attention equation is given by: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V, where Q, K, and V are the query, key, and value vectors respectively, and d is the dimensionality of the vectors.\\n* The role of query, key, and value vectors is crucial in self attention. The query vector represents the context in which the attention is being applied, the key vector represents the information being attended to, and the value vector represents the information being retrieved.\\n* The importance of scaling in self attention cannot be overstated. The scaling factor, sqrt(d), is used to prevent the dot product of the query and key vectors from growing too large, which can lead to extremely small gradients and make the model difficult to train. This scaling factor helps to stabilize the training process and improve the overall performance of the model. By understanding the mathematical formulation of self attention, developers can better appreciate the intricacies of the Transformer architecture and design more effective models for their applications.'),\n",
       "  (3,\n",
       "   '## Implementing Self Attention\\nTo implement self attention in a transformer model, you need to understand the key components involved. The self attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\\n\\n*   A minimal code sketch for self attention implementation can be provided using the PyTorch library:\\n    ```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim, num_heads):\\n        super(SelfAttention, self).__init__()\\n        self.embed_dim = embed_dim\\n        self.num_heads = num_heads\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        # Get the query, key, and value vectors\\n        Q = self.query_linear(x)\\n        K = self.key_linear(x)\\n        V = self.value_linear(x)\\n\\n        # Compute the attention scores\\n        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.embed_dim)\\n\\n        # Compute the weighted sum\\n        output = torch.matmul(attention_scores, V)\\n        return output\\n```\\n*   Masking is crucial in self attention to prevent the model from attending to future tokens in the input sequence, which can lead to information leakage. This is particularly important in tasks like language translation, where the model should only attend to the tokens that have been generated so far.\\n*   Multi-head attention is a technique used to extend the self attention mechanism by allowing the model to jointly attend to information from different representation subspaces at different positions. This is achieved by applying multiple attention mechanisms in parallel, each with a different set of learnable weights. The outputs from each attention mechanism are then concatenated and linearly transformed to produce the final output. This helps the model to capture a wider range of contextual relationships in the input sequence. \\n\\nIn the context of transformer models, self attention is a key component that enables the model to capture long-range dependencies and contextual relationships in the input sequence. By implementing self attention using the provided code sketch and incorporating masking and multi-head attention, you can build a robust transformer model that achieves state-of-the-art results in various natural language processing tasks.'),\n",
       "  (4,\n",
       "   \"## Edge Cases and Failure Modes\\nSelf attention in Transformer architecture can be sensitive to certain edge cases and failure modes. The performance of self attention can be impacted by various factors, including sequence length and input sparsity.\\n\\n* The impact of sequence length on self attention is significant, as longer sequences can lead to increased computational complexity and memory requirements. This can result in slower processing times and higher memory usage, making it challenging to handle long sequences.\\n* Sparse input can also affect self attention, as it can lead to a decrease in the model's ability to capture meaningful relationships between input elements. This is because self attention relies on the interactions between input elements to compute attention weights, and sparse input can reduce the number of interactions.\\n* The failure modes of self attention can be attributed to several factors, including the inability to capture local relationships, sensitivity to input order, and vulnerability to adversarial attacks. Not found in provided sources.\"),\n",
       "  (5,\n",
       "   '## Performance and Cost Considerations\\nThe self-attention mechanism in Transformer architecture has significant implications for performance and cost. \\n* Computational complexity is a key consideration: self attention has a time complexity of O(n^2), where n is the sequence length, due to the dot-product attention mechanism. \\n* Memory requirements are also substantial, as self attention requires storing attention weights and intermediate results, which can lead to high memory usage for long sequences. \\n* To mitigate these costs, several optimization techniques can be employed, including:\\n  + Using attention with sparse matrices to reduce the number of computations required\\n  + Implementing attention with hierarchical or local attention patterns to reduce the sequence length\\n  + Utilizing model parallelism or tensor parallelism to split the computation across multiple devices or GPUs. \\nThese optimization techniques can help reduce the computational and memory requirements of self attention, making it more efficient and scalable for large-scale applications. \\nOverall, understanding the performance and cost considerations of self attention is crucial for designing and deploying efficient Transformer-based models.'),\n",
       "  (6,\n",
       "   \"## Security and Privacy Considerations\\nSelf attention in transformer architecture poses potential security risks, including data exposure and model exploitation. The mechanism's ability to attend to different parts of the input sequence can be vulnerable to attacks, such as data poisoning or model inversion. \\n* Data exposure can occur when sensitive information is not properly masked or sanitized, allowing unauthorized access to confidential data.\\n* Model exploitation can happen when an attacker manipulates the input to influence the model's output, potentially leading to biased or incorrect results.\\n\\nData privacy is crucial in self attention, as the mechanism processes sensitive information. Ensuring the confidentiality, integrity, and availability of data is essential to prevent unauthorized access or misuse. \\n\\nTo mitigate security and privacy risks, several techniques can be employed, including:\\n* Implementing robust access controls and authentication mechanisms\\n* Using secure data storage and transmission protocols\\n* Regularly updating and patching the model to prevent vulnerabilities\\n* Utilizing differential privacy techniques to protect sensitive information\\n* Conducting thorough risk assessments and testing to identify potential weaknesses.\"),\n",
       "  (7,\n",
       "   \"## Debugging and Observability Tips\\nTo effectively debug and observe self-attention in transformer models, several strategies can be employed. \\n* Provide tips for debugging self attention: When debugging self-attention, it's essential to check the input and output shapes of the attention mechanism, ensuring they match the expected dimensions. Additionally, verifying the correct application of masking to prevent attending to future tokens is crucial.\\n* Explain the importance of visualization in self attention: Visualization plays a vital role in understanding self-attention. By visualizing the attention weights, developers can gain insights into which parts of the input sequence the model is focusing on, helping identify potential issues or biases in the model.\\n* Describe the use of logging and monitoring in self attention: Logging and monitoring are also crucial for debugging self-attention. By logging attention weights and other relevant metrics, developers can track the model's behavior during training and inference, identifying potential issues before they become critical. Monitoring tools can also help detect anomalies in attention patterns, allowing for prompt intervention and correction. \\nBy combining these strategies, developers can effectively debug and observe self-attention in transformer models, leading to better model performance and reliability.\"),\n",
       "  (1,\n",
       "   '## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, allowing the model to focus on specific parts of the input data when generating outputs. It is defined as a mechanism that attends to different positions of a single sequence, weighing their importance, and computes a representation of the sequence based on this weighting. The purpose of self attention is to enable the model to capture long-range dependencies and contextual relationships within the input data.\\n\\nIn contrast to traditional attention mechanisms, self attention differs in that it does not rely on a separate encoder-decoder structure. Instead, self attention operates on a single sequence, allowing the model to attend to different parts of the input data simultaneously. This differs from traditional attention mechanisms, which typically attend to a separate context or memory.\\n\\nThe benefits of using self attention in transformer models are numerous. It enables the model to capture complex contextual relationships and dependencies within the input data, leading to improved performance on tasks such as language translation and text generation. Additionally, self attention allows the model to handle variable-length input sequences, making it well-suited for tasks such as machine translation and text summarization. Overall, self attention is a powerful tool that has revolutionized the field of natural language processing and has become a staple component of transformer architecture.'),\n",
       "  (2,\n",
       "   '## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand how self attention works, we need to derive the self attention equation. The self attention equation is derived from the concept of attention, which is calculated as the weighted sum of the value vectors, where the weights are computed based on the similarity between the query and key vectors.\\n\\n* The self attention equation is given by: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V, where Q, K, and V are the query, key, and value vectors respectively, and d is the dimensionality of the vectors.\\n* The role of query, key, and value vectors is crucial in self attention. The query vector represents the context in which the attention is being applied, the key vector represents the information being attended to, and the value vector represents the information being retrieved.\\n* The importance of scaling in self attention cannot be overstated. The scaling factor, sqrt(d), is used to prevent the dot product of the query and key vectors from growing too large, which can lead to extremely small gradients and make the model difficult to train. This scaling factor helps to stabilize the training process and improve the overall performance of the model. By understanding the mathematical formulation of self attention, developers can better appreciate the intricacies of the Transformer architecture and design more effective models for their applications.'),\n",
       "  (3,\n",
       "   '## Implementing Self Attention\\nTo implement self attention in a transformer model, you need to understand the key components involved. The self attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\\n\\n*   A minimal code sketch for self attention implementation can be provided using the PyTorch library:\\n    ```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim, num_heads):\\n        super(SelfAttention, self).__init__()\\n        self.embed_dim = embed_dim\\n        self.num_heads = num_heads\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        # Get the query, key, and value vectors\\n        Q = self.query_linear(x)\\n        K = self.key_linear(x)\\n        V = self.value_linear(x)\\n\\n        # Compute the attention scores\\n        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.embed_dim)\\n\\n        # Compute the weighted sum\\n        output = torch.matmul(attention_scores, V)\\n        return output\\n```\\n*   Masking is crucial in self attention to prevent the model from attending to future tokens in the input sequence, which can lead to information leakage. This is particularly important in tasks like language translation, where the model should only attend to the tokens that have been generated so far.\\n*   Multi-head attention is a technique used to extend the self attention mechanism by allowing the model to jointly attend to information from different representation subspaces at different positions. This is achieved by applying multiple attention mechanisms in parallel, each with a different set of learnable weights. The outputs from each attention mechanism are then concatenated and linearly transformed to produce the final output. This helps the model to capture a wider range of contextual relationships in the input sequence. \\n\\nIn the context of transformer models, self attention is a key component that enables the model to capture long-range dependencies and contextual relationships in the input sequence. By implementing self attention using the provided code sketch and incorporating masking and multi-head attention, you can build a robust transformer model that achieves state-of-the-art results in various natural language processing tasks.'),\n",
       "  (4,\n",
       "   \"## Edge Cases and Failure Modes\\nSelf attention in Transformer architecture can be sensitive to certain edge cases and failure modes. The performance of self attention can be impacted by various factors, including sequence length and input sparsity.\\n\\n* The impact of sequence length on self attention is significant, as longer sequences can lead to increased computational complexity and memory requirements. This can result in slower processing times and higher memory usage, making it challenging to handle long sequences.\\n* Sparse input can also affect self attention, as it can lead to a decrease in the model's ability to capture meaningful relationships between input elements. This is because self attention relies on the interactions between input elements to compute attention weights, and sparse input can reduce the number of interactions.\\n* The failure modes of self attention can be attributed to several factors, including the inability to capture local relationships, sensitivity to input order, and vulnerability to adversarial attacks. Not found in provided sources.\"),\n",
       "  (5,\n",
       "   '## Performance and Cost Considerations\\nThe self-attention mechanism in Transformer architecture has significant implications for performance and cost. \\n* Computational complexity is a key consideration: self attention has a time complexity of O(n^2), where n is the sequence length, due to the dot-product attention mechanism. \\n* Memory requirements are also substantial, as self attention requires storing attention weights and intermediate results, which can lead to high memory usage for long sequences. \\n* To mitigate these costs, several optimization techniques can be employed, including:\\n  + Using attention with sparse matrices to reduce the number of computations required\\n  + Implementing attention with hierarchical or local attention patterns to reduce the sequence length\\n  + Utilizing model parallelism or tensor parallelism to split the computation across multiple devices or GPUs. \\nThese optimization techniques can help reduce the computational and memory requirements of self attention, making it more efficient and scalable for large-scale applications. \\nOverall, understanding the performance and cost considerations of self attention is crucial for designing and deploying efficient Transformer-based models.'),\n",
       "  (6,\n",
       "   \"## Security and Privacy Considerations\\nSelf attention in transformer architecture poses potential security risks, including data exposure and model exploitation. The mechanism's ability to attend to different parts of the input sequence can be vulnerable to attacks, such as data poisoning or model inversion. \\n* Data exposure can occur when sensitive information is not properly masked or sanitized, allowing unauthorized access to confidential data.\\n* Model exploitation can happen when an attacker manipulates the input to influence the model's output, potentially leading to biased or incorrect results.\\n\\nData privacy is crucial in self attention, as the mechanism processes sensitive information. Ensuring the confidentiality, integrity, and availability of data is essential to prevent unauthorized access or misuse. \\n\\nTo mitigate security and privacy risks, several techniques can be employed, including:\\n* Implementing robust access controls and authentication mechanisms\\n* Using secure data storage and transmission protocols\\n* Regularly updating and patching the model to prevent vulnerabilities\\n* Utilizing differential privacy techniques to protect sensitive information\\n* Conducting thorough risk assessments and testing to identify potential weaknesses.\"),\n",
       "  (7,\n",
       "   \"## Debugging and Observability Tips\\nTo effectively debug and observe self-attention in transformer models, several strategies can be employed. \\n* Provide tips for debugging self attention: When debugging self-attention, it's essential to check the input and output shapes of the attention mechanism, ensuring they match the expected dimensions. Additionally, verifying the correct application of masking to prevent attending to future tokens is crucial.\\n* Explain the importance of visualization in self attention: Visualization plays a vital role in understanding self-attention. By visualizing the attention weights, developers can gain insights into which parts of the input sequence the model is focusing on, helping identify potential issues or biases in the model.\\n* Describe the use of logging and monitoring in self attention: Logging and monitoring are also crucial for debugging self-attention. By logging attention weights and other relevant metrics, developers can track the model's behavior during training and inference, identifying potential issues before they become critical. Monitoring tools can also help detect anomalies in attention patterns, allowing for prompt intervention and correction. \\nBy combining these strategies, developers can effectively debug and observe self-attention in transformer models, leading to better model performance and reliability.\")],\n",
       " 'merged_md': \"# Self Attention in Transformer Architecture\\n\\n## Introduction to Self Attention\\nSelf attention is a key component in transformer architecture, allowing the model to focus on specific parts of the input data when generating outputs. It is defined as a mechanism that attends to different positions of a single sequence, weighing their importance, and computes a representation of the sequence based on this weighting. The purpose of self attention is to enable the model to capture long-range dependencies and contextual relationships within the input data.\\n\\nIn contrast to traditional attention mechanisms, self attention differs in that it does not rely on a separate encoder-decoder structure. Instead, self attention operates on a single sequence, allowing the model to attend to different parts of the input data simultaneously. This differs from traditional attention mechanisms, which typically attend to a separate context or memory.\\n\\nThe benefits of using self attention in transformer models are numerous. It enables the model to capture complex contextual relationships and dependencies within the input data, leading to improved performance on tasks such as language translation and text generation. Additionally, self attention allows the model to handle variable-length input sequences, making it well-suited for tasks such as machine translation and text summarization. Overall, self attention is a powerful tool that has revolutionized the field of natural language processing and has become a staple component of transformer architecture.\\n\\n## Mathematical Formulation of Self Attention\\nThe self attention mechanism is a core component of the Transformer architecture, allowing the model to attend to different parts of the input sequence simultaneously. To understand how self attention works, we need to derive the self attention equation. The self attention equation is derived from the concept of attention, which is calculated as the weighted sum of the value vectors, where the weights are computed based on the similarity between the query and key vectors.\\n\\n* The self attention equation is given by: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V, where Q, K, and V are the query, key, and value vectors respectively, and d is the dimensionality of the vectors.\\n* The role of query, key, and value vectors is crucial in self attention. The query vector represents the context in which the attention is being applied, the key vector represents the information being attended to, and the value vector represents the information being retrieved.\\n* The importance of scaling in self attention cannot be overstated. The scaling factor, sqrt(d), is used to prevent the dot product of the query and key vectors from growing too large, which can lead to extremely small gradients and make the model difficult to train. This scaling factor helps to stabilize the training process and improve the overall performance of the model. By understanding the mathematical formulation of self attention, developers can better appreciate the intricacies of the Transformer architecture and design more effective models for their applications.\\n\\n## Implementing Self Attention\\nTo implement self attention in a transformer model, you need to understand the key components involved. The self attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\\n\\n*   A minimal code sketch for self attention implementation can be provided using the PyTorch library:\\n    ```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_dim, num_heads):\\n        super(SelfAttention, self).__init__()\\n        self.embed_dim = embed_dim\\n        self.num_heads = num_heads\\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\\n\\n    def forward(self, x):\\n        # Get the query, key, and value vectors\\n        Q = self.query_linear(x)\\n        K = self.key_linear(x)\\n        V = self.value_linear(x)\\n\\n        # Compute the attention scores\\n        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.embed_dim)\\n\\n        # Compute the weighted sum\\n        output = torch.matmul(attention_scores, V)\\n        return output\\n```\\n*   Masking is crucial in self attention to prevent the model from attending to future tokens in the input sequence, which can lead to information leakage. This is particularly important in tasks like language translation, where the model should only attend to the tokens that have been generated so far.\\n*   Multi-head attention is a technique used to extend the self attention mechanism by allowing the model to jointly attend to information from different representation subspaces at different positions. This is achieved by applying multiple attention mechanisms in parallel, each with a different set of learnable weights. The outputs from each attention mechanism are then concatenated and linearly transformed to produce the final output. This helps the model to capture a wider range of contextual relationships in the input sequence. \\n\\nIn the context of transformer models, self attention is a key component that enables the model to capture long-range dependencies and contextual relationships in the input sequence. By implementing self attention using the provided code sketch and incorporating masking and multi-head attention, you can build a robust transformer model that achieves state-of-the-art results in various natural language processing tasks.\\n\\n## Edge Cases and Failure Modes\\nSelf attention in Transformer architecture can be sensitive to certain edge cases and failure modes. The performance of self attention can be impacted by various factors, including sequence length and input sparsity.\\n\\n* The impact of sequence length on self attention is significant, as longer sequences can lead to increased computational complexity and memory requirements. This can result in slower processing times and higher memory usage, making it challenging to handle long sequences.\\n* Sparse input can also affect self attention, as it can lead to a decrease in the model's ability to capture meaningful relationships between input elements. This is because self attention relies on the interactions between input elements to compute attention weights, and sparse input can reduce the number of interactions.\\n* The failure modes of self attention can be attributed to several factors, including the inability to capture local relationships, sensitivity to input order, and vulnerability to adversarial attacks. Not found in provided sources.\\n\\n## Performance and Cost Considerations\\nThe self-attention mechanism in Transformer architecture has significant implications for performance and cost. \\n* Computational complexity is a key consideration: self attention has a time complexity of O(n^2), where n is the sequence length, due to the dot-product attention mechanism. \\n* Memory requirements are also substantial, as self attention requires storing attention weights and intermediate results, which can lead to high memory usage for long sequences. \\n* To mitigate these costs, several optimization techniques can be employed, including:\\n  + Using attention with sparse matrices to reduce the number of computations required\\n  + Implementing attention with hierarchical or local attention patterns to reduce the sequence length\\n  + Utilizing model parallelism or tensor parallelism to split the computation across multiple devices or GPUs. \\nThese optimization techniques can help reduce the computational and memory requirements of self attention, making it more efficient and scalable for large-scale applications. \\nOverall, understanding the performance and cost considerations of self attention is crucial for designing and deploying efficient Transformer-based models.\\n\\n## Security and Privacy Considerations\\nSelf attention in transformer architecture poses potential security risks, including data exposure and model exploitation. The mechanism's ability to attend to different parts of the input sequence can be vulnerable to attacks, such as data poisoning or model inversion. \\n* Data exposure can occur when sensitive information is not properly masked or sanitized, allowing unauthorized access to confidential data.\\n* Model exploitation can happen when an attacker manipulates the input to influence the model's output, potentially leading to biased or incorrect results.\\n\\nData privacy is crucial in self attention, as the mechanism processes sensitive information. Ensuring the confidentiality, integrity, and availability of data is essential to prevent unauthorized access or misuse. \\n\\nTo mitigate security and privacy risks, several techniques can be employed, including:\\n* Implementing robust access controls and authentication mechanisms\\n* Using secure data storage and transmission protocols\\n* Regularly updating and patching the model to prevent vulnerabilities\\n* Utilizing differential privacy techniques to protect sensitive information\\n* Conducting thorough risk assessments and testing to identify potential weaknesses.\\n\\n## Debugging and Observability Tips\\nTo effectively debug and observe self-attention in transformer models, several strategies can be employed. \\n* Provide tips for debugging self attention: When debugging self-attention, it's essential to check the input and output shapes of the attention mechanism, ensuring they match the expected dimensions. Additionally, verifying the correct application of masking to prevent attending to future tokens is crucial.\\n* Explain the importance of visualization in self attention: Visualization plays a vital role in understanding self-attention. By visualizing the attention weights, developers can gain insights into which parts of the input sequence the model is focusing on, helping identify potential issues or biases in the model.\\n* Describe the use of logging and monitoring in self attention: Logging and monitoring are also crucial for debugging self-attention. By logging attention weights and other relevant metrics, developers can track the model's behavior during training and inference, identifying potential issues before they become critical. Monitoring tools can also help detect anomalies in attention patterns, allowing for prompt intervention and correction. \\nBy combining these strategies, developers can effectively debug and observe self-attention in transformer models, leading to better model performance and reliability.\\n\",\n",
       " 'md_with_placeholders': '',\n",
       " 'image_specs': [],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"Self Attention in Transformer Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9022798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
